---
title: "Toronto Police Break and Enter Data Analysis"
author: "Adele, Wayne, Alexandra, Andrei & Cindy(Shih-Ting), TUT0201, Group 4"
subtitle: "Studying Crime Trends in Neighborhoods with high B&E rates"
date: March 30, 2020
output: 
  beamer_presentation:
    theme: "Pittsburgh"
    colortheme: "orchid"
    fonttheme: "serif"
    slide_level: 2
---



```{r, echo=FALSE, message=FALSE, warning=FALSE}
# echo=FALSE will stop the code chunk from appearing in the knit document
# warning=FALSE and message=FALSE will stop R messages from appearing in the knit document
library(tidyverse)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load the TPS break in data
break_and_enters <- read_csv("break_and_enters.csv")
```


## Introduction

The offence of break and enter encompasses situations where individuals tresspass or attempt to trespass on private enclosed property. 

Though break and enters are not the most violent crimes, the preventative measures for such offenses can be exhaustive and costly--alarm systems, surveillance cameras, security teams, etc. In order to ensure adequate protection of vulnerable spaces and appropriate usage of resources, it is important to understand the spatial and temporal patterns of break and enters. 

The data used to analyze these patterns comes from the Toronto Police Service, and includes reported break and enters from 2014-2019. 

## Objectives

Our main question which we seek to answer is: **"In neighborhoods with high B&E rates, what are the commonalities between the crimes?"** 
We then divided this question into a series of smaller subquestions, focused on specific variables:

* Are particular premise types more susceptible to B&E’s in high-risk compared to low-risk neighborhoods? 

* Is there a particular day of the week or hour of the day that inhabitants should be more wary of B&E’s? 

* Is there a certain kind of B&E that occurs more often in these high-rate neighborhoods?

## Objectives Continued
Using the data given to us by the TPS, we want to understand commonalities between offenses in the neighborhoods that experience the highest proportion of break-and-enters. The purpose of doing so is to help the TPS understand the characteristics of B&E offences in vulnerable neighborhoods in order to allocate officers and other resources efficiently to reduce crime across the city. 
Once these strategies are implemented, it is likely that the city will change and different neighborhoods will become hotbeds for B&E's. Hopefully, once a strategy is developed for high-risk neighborhoods, it can be transplanted and altered slightly to work elsewhere.  

We hypothesize that premise type, as well as temporal trends such as time of day/day of the week, influences vulnerability in high-risk neighborhoods. Furthermore, we expect that these neighborhoods might experience a higher density of B&E with intent. 

## Data Summary 

\tiny We first created a new variable for each offence based off of the neighborhood it occurred in, called *prop*, which represented the proportion of all break and enters that occurred in that neighborhood. We then created a boxplot and a corresponding summary table to help us understand how the proportions were distributed. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# echo=FALSE will stop the code chunk from appearing in the knit document
# warning=FALSE and message=FALSE will stop R messages from appearing in the knit document
grouped_hood <- break_and_enters %>% 
  filter(!is.na(Neighbourhood)) %>%
  group_by(Neighbourhood) %>%
  mutate(number = n()) %>%
  mutate(prop = number/43302)
break_and_enters_prop <- merge(break_and_enters,grouped_hood,by=c("X1", "Index", "event_unique_id", "occurrencedate", "reporteddate", "premisetype", "offence", "reportedyear", "reportedmonth", "reportedday", "reporteddayofyear", "reporteddayofweek", "reportedhour", "occurrenceyear", "occurrencemonth", "occurrenceday", "occurrencedayofyear", "occurrencedayofweek", "occurrencehour", "MCI", "Division", "Hood_ID", "Long", "Lat"))
```
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=1.5}
ggplot(break_and_enters_prop,
aes(x = "", y=prop)) + xlab("Proportion of total break-ins that occurred in observation's 
                            neighborhood") +
geom_boxplot()
```
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=0.25, fig.height=0.25}
break_and_enters_prop %>%
summarise(
Q1=quantile(prop, 0.25),
med=median(prop),
Q3=quantile(prop, 0.75),
max=max(prop))
```

\tiny We were interested in the extremes of this data--in other words, neighborhoods that were high-risk and neighborhoods that were low-risk. This allows us look at data that is "extreme"--those with proportions <= the first quartile, and those >= the third (aka, the bottom 25% and the top 25% neighborhoods).


## Statistical Methods

<<<<<<< HEAD
Describe here what you have done to the data without presenting any results (output). If you want to indicate variables by symbols or variable names, define them here. 
Subquestion 1: Hypothesis Test for Difference in Proportions and Linear Regression
Subquestion 2:
Subquestion 3: Bar Graph for Identifying Most Common Offence in High-Rate Neighborhoods, Hypothesis Test for Difference in Proportions, Multi-Linear Regression to predict the proportion of Break and Enters using the offence types and level as predictors. 
=======
\tiny 
*Subquestion 1:* 

* Hypothesis Test for Difference in Proportions of Commercial Break-ins and Linear Regression  

*Subquestion 2:*  
*Subquestion 3:* 

* Bar Graph: By examining the data specifically for high-risk neighborhoods, we identify which offence type appears the most and hence occurs the most often. 
>>>>>>> 6491e16599d54f48a1143b6b081a5d8720b2fec1

* Hypothesis Test for Difference in Proportions: We randomly shuffled the values in the original data without replacement and got their distribution to see if the observed difference in proportion of B&E offences is "normal" under the assumption that there's no proportion difference between high-risk and low-risk neighborhoods. 

* 

## Results--Subquestion 1: Hypothesis Test

\tiny Upon initial observation, we noticed that the proportion of commercial break-ins in high-risk neighborhoods appeared much larger than commercial break-ins in low-risk neighborhoods. This was tested using a difference of proportions hypothesis test, using the following hypotheses: 

$H_0:p_H = p_L$ & $H_A:p_H \neq p_L$,
where $p_H , p_L$ are the proportion of commercial break-ins that happened in high- and low-risk neighborhoods, respectively. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=0, fig.height=0}
b_and_e_top <- break_and_enters_prop %>%
  filter(prop >= 0.01457208)
b_and_e_lower <- break_and_enters_prop %>%
  filter(prop <= 0.005565563)
b_and_e_extremes <- rbind(b_and_e_top, b_and_e_lower)
b_and_e_extremes <- b_and_e_extremes %>% mutate(level = ifelse(prop >= 0.01457208, "high", "low"))
set.seed(100)
prop_data <- b_and_e_extremes %>% group_by(level) %>%
summarise(n_commercial = sum(premisetype=="Commercial"),
n=n(),
prop_commercial = n_commercial / n)
test_stat <- prop_data %>%
summarise(test_stat = diff(prop_commercial)) %>%
as.numeric()
repetitions <- 1000; # number of repetitions (i.e. possible groupings)
simulated_values <- rep(NA, repetitions); # empty vector for sim. values
for(i in 1:repetitions){
simdata <- b_and_e_extremes %>%
mutate(level = sample(level)) %>%
group_by(level) %>%
summarise(n=n(),
n_commercial = sum(premisetype=="Commercial"),
prop_commercial = n_commercial / n)
sim_prop_diff <- simdata %>% summarise(value = diff(prop_commercial))
simulated_values[i] <- as.numeric(sim_prop_diff)
}
sim <- tibble(prop_diff = simulated_values)

b_and_e_extremes %>% group_by(level) %>%
summarise(n=n(),
n_commercial = sum(premisetype=="Commercial"),
prop_commercial = n_commercial / n)
prop_data <- b_and_e_extremes %>% group_by(level) %>%
summarise(n_commercial = sum(premisetype=="Commercial"),
n=n(),
prop_commercial = n_commercial / n)
test_stat <- prop_data %>%
summarise(test_stat = diff(prop_commercial)) %>%
as.numeric()
test_stat

```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=1.5}
ggplot(sim, aes(x=prop_diff)) +
geom_histogram(binwidth = 0.005, fill="gray", color="black") +
labs(x = "Difference in proportion of commercial breakins for H/LR neighborhoods, 
     assuming no difference between groups") + 
  geom_vline(xintercept=c(test_stat, -test_stat), color="red")
```

Given the low p value of p < 0.0001, we can conclude that there is strong evidence against the null that states that $p_H = p_L$. Therefore, we have reason to believe that the proportion of commercial breakins in these high-rate neighborhoods is different than low-rate neighborhoods. 

## Results--Subquestion 1: Linear Regression

\tiny This same question can be analyzed through a different lense: linear regression. For this method, a new variable **residential** was created that took the value *"Yes"* if the premise was a house or apartment, and *"No"* if it was anything else. 

\tiny The equation we are testing is $prop_i = \beta_0 + \beta_1 I(premise\ is\ residential) + \epsilon_i$. In other words, we are seeing if whether a property is residential makes it more vulnerable to break-ins (vulnerability approximated by **prop**)
We are interested in testing $H_0: \beta_1 = 0$ vs $H_A: \beta_1 \neq 0$

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=1}
b_and_e_top <- b_and_e_top %>% mutate(residential = ifelse(premisetype == "House" | premisetype == "Apartment", "Yes", "No")) 
summary(lm(prop ~ residential, data=b_and_e_top))$coefficients
```
Based on the fitted regression model, the p-value corresponding to this hypothesis test is very small (the
estimated p-value from R is 0.001258903), so we have very strong evidence to reject the null hypothesis and
conclude that there is an association between a property being residential and proportion of neighborhood breakins, and the negative coefficient for $\beta_1$ suggests that if a property **is** residential, the **vulnerability** of that property will be lower than if it isn't residential. 

## Results--Subquestion 1: Linear Regression RMSE 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=1}
set.seed(1210); n <- nrow(b_and_e_top)
training_indices <- sample(1:n, size=round(0.8*n))
train <- b_and_e_top[training_indices,]; y_train <- train$prop;
# Testing dataset includes all observations NOT in the training data
test <- b_and_e_top[-training_indices,]; y_test <- test$prop;
# Fit models to training data
modB_train <- lm(prop ~ residential, data=train)
# Make predictions for testing data using training model
yhat_modB_test <- predict(modB_train, newdata = test)
# Make predictions for training data using training model
yhat_modB_train <- predict(modB_train, newdata = train)
# Calculate RMSE for testing data
modB_test_RMSE <- sqrt(sum((y_test - yhat_modB_test)^2) / nrow(test))
# Calculate RMSE for training data
modB_train_RMSE <- sqrt(sum((y_train - yhat_modB_train)^2) / nrow(train))
tibble(
RMSE_testdata = c(modB_test_RMSE),
RMSE_traindata = c(modB_train_RMSE),
ratio_of_RMSEs = RMSE_traindata / RMSE_testdata)
```

The Root Mean Squared Error measures prediction error for predictions from a linear regression model. Splitting the data up into testing (20%) and training (80%), we see that the RSME for both is around 0.006. Therefore, on average, predicted **vulnerabilities** (or proportions) are about 0.006 away from the true value in this data set restricted to high-risk neighborhoods. 

The Root Mean Squared Error measures prediction error for predictions from a linear regression model. Splitting the data up into testing (20%) and training (80%), we see that the RSME for both is around 0.006. In other words, the prediction error for this model is 0.006. 


## Results--Subquestion 2: Frequency of Occurrence Year 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=1}
# these lines are just some prerequisite variables referenced by Q2
break_and_enters <- break_and_enters %>% filter(!is.na(Neighbourhood))
neighbourhoods <- table(break_and_enters$Neighbourhood)
neighbourhoods <- data.frame(neighbourhoods) 
colnames(neighbourhoods) <- c("Neighbourhood", "Frequency")
neighbourhoods <- neighbourhoods[order(neighbourhoods$Frequency, decreasing = TRUE),]  

target_hoods <- head(neighbourhoods, 35)

break_and_enters <- break_and_enters %>% 
  mutate(targeted = ifelse(Neighbourhood %in% target_hoods$Neighbourhood, "Bottom 25%", "Top 75%"))

top25 <- break_and_enters %>%
  filter(Neighbourhood %in% target_hoods$Neighbourhood)

bottom75 <- break_and_enters %>%
  filter(!Neighbourhood %in% target_hoods$Neighbourhood)

diff_factor = nrow(bottom75) / nrow(top25) 
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2}
targeted <- break_and_enters %>% 
  group_by(targeted) %>% 
  filter(!is.na(occurrenceyear))

ggplot(targeted, aes(x = occurrenceyear)) +
  geom_bar(colour = "black", fill = "blue", alpha = 0.2) +
  facet_wrap(~targeted) +
  xlab("Year") +
  ylab("Occurrences")

targeted <- break_and_enters %>% 
  group_by(targeted) %>% 
  filter(!is.na(occurrencemonth))

targeted$occurrencemonth <- ordered(targeted$occurrencemonth, levels = month.name)

ggplot(targeted, aes(x = occurrencemonth)) +
  geom_bar(colour = "black", fill = "blue", alpha = 0.2) +
  facet_wrap(~targeted) +
  xlab("Month") +
  ylab("Number of Occurrences") +
  scale_x_discrete(label = month.abb)
```

## Results--Subquestion 2: 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=3}
targeted <- break_and_enters %>% 
  group_by(targeted) %>% 
  filter(!is.na(occurrencedayofweek))

day.abb <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
day.name <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")

targeted$occurrencedayofweek <- ordered(targeted$occurrencedayofweek, levels = day.name)

ggplot(targeted, aes(x = occurrencedayofweek)) +
  geom_bar(colour = "black", fill = "blue", alpha = 0.2) +
  facet_wrap(~targeted) +
  xlab("Day of the Week") +
  ylab("Number of Occurrences") +
  scale_x_discrete(label = day.abb)

targeted <- break_and_enters %>% 
  group_by(targeted) %>% 
  filter(!is.na(occurrencehour))

ggplot(targeted, aes(x = occurrencehour)) +
  geom_bar(colour = "black", fill = "blue", alpha = 0.2) +
  facet_wrap(~targeted) +
  xlab("Hour of the Day") +
  ylab("Number of Occurrences")
```

## Subquestion 2--Are these trends any different?
Are discreptencies in between trends in our two groups of neighbourhoods significant? Do our two groups have the same trends, or do the neighbourhoods with more frequent B&Es show different time trends?

*Hypothesis Test*:  
$H_0$: There is no difference in the patterns describing when B&Es occur in our worst-affected neighbourhoods.  
$H_a$: The patterns describing when B&Es occur are different in our worst-affected neighbourhoods.  

For the sake of simplicity, we will only analyze occurrence month and hour of day.

## Subquestion 2--Occurrences by Month
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=3}
table25 <- table(top25$occurrencemonth)
table25 <- data.frame(table25)
table25 <- table25[order(match(table25$Var1, month.name)), ] %>%
  mutate(change = (Freq - lag(Freq)) / lag(Freq) * 100)

table75 <- table(bottom75$occurrencemonth)
table75 <- data.frame(table75)
table75 <- table75[order(match(table75$Var1, month.name)), ] %>%
  mutate(change = (Freq - lag(Freq)) / lag(Freq) * 100)

delta <- abs(table75$change - table25$change)
delta <- tibble(delta)
delta <- delta %>% filter(!is.na(delta))

test_stat <- sum(delta)

set.seed(123)
repetitions <- 1000; # number of repetitions (i.e. possible groupings)
simulated_values <- rep(NA, repetitions); # empty vector for sim. values

for(i in 1:repetitions){
  
  simdata <- sample_n(break_and_enters, 21142, replace = TRUE)
  simdata <- table(simdata$occurrencemonth)
  simdata <- data.frame(simdata)
  simdata <- simdata[order(match(simdata$Var1, month.name)), ] %>%
    mutate(change = (Freq - lag(Freq)) / lag(Freq) * 100)
  
  delta <- abs(table75$change - simdata$change)
  delta <- tibble(delta)
  delta <- delta %>% filter(!is.na(delta))
  simulated_values[i] <- sum(delta)

}

simulated_values <- tibble(value = simulated_values)
median <- median(simulated_values$value)
  
simulated_values %>% ggplot(aes(x=value)) +  
  geom_histogram(colour = "black", fill = "grey") +  
  geom_vline(xintercept = median  - abs(median - test_stat), color = "red") +  
  geom_vline(xintercept = median + abs(median - test_stat), color = "blue") +
  xlab("Differences in Monthly Change") +
  ylab("Frequency")

pvalue <- simulated_values %>%  
  filter(abs(value - median) >= abs(test_stat - median)) %>%  
  summarise(p_value = n() / repetitions)
```
With our test stat of 47.04 units of discreptency, we get a p-value of 0.236, suggesting no evidence against the null hypothesis. In other words, **monthly trends likely remain consistent in the most vulnerable neighbourhoods.**

## Subquestion 2--Occurrences by Hour of Day
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=3}
table25 <- table(top25$occurrencehour)
table25 <- data.frame(table25)
table25 <- table25[order(match(table25$Var1, month.name)), ] %>%
  mutate(change = (Freq - lag(Freq)) / lag(Freq) * 100)

table75 <- table(bottom75$occurrencehour)
table75 <- data.frame(table75)
table75 <- table75[order(match(table75$Var1, month.name)), ] %>%
  mutate(change = (Freq - lag(Freq)) / lag(Freq) * 100)

delta <- abs(table75$change - table25$change)
delta <- tibble(delta)
delta <- delta %>% filter(!is.na(delta))

test_stat <- sum(delta)

repetitions <- 1000; # number of repetitions (i.e. possible groupings)
simulated_values <- rep(NA, repetitions); # empty vector for sim. values

for(i in 1:repetitions){
  
  simdata <- sample_n(break_and_enters, 21142, replace = TRUE)
  simdata <- table(simdata$occurrencehour)
  simdata <- data.frame(simdata)
  simdata <- simdata[order(match(simdata$Var1, month.name)), ] %>%
    mutate(change = (Freq - lag(Freq)) / lag(Freq) * 100)
  
  delta <- abs(table75$change - simdata$change)
  delta <- tibble(delta)
  delta <- delta %>% filter(!is.na(delta))
  simulated_values[i] <- sum(delta)

}

simulated_values <- tibble(value = simulated_values)
median <- median(simulated_values$value)
  
simulated_values %>% ggplot(aes(x=value)) +  
  geom_histogram(colour = "black", fill = "grey") +  
  geom_vline(xintercept = median  - abs(median - test_stat), color = "red") +  
  geom_vline(xintercept = median + abs(median - test_stat), color = "blue") +
  xlab("Differences in Hourly Change") +
  ylab("Frequency")

pvalue <- simulated_values %>%  
  filter(abs(value - median) >= abs(test_stat - median)) %>%  
  summarise(p_value = n() / repetitions)
```
Again, with our test stat of 232.69 units of discreptency, we get a p-value of 0, suggesting strong evidence against the null hypothesis. In other words, **hourly trends in the most vulnerable neighbourhoods are very likely different in a significant way.**

## Results--Subquestion 3: 
From the bar graph below, we can clearly identify that **B&E offences** occur the **most often** in high-risk neighborhoods.  

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2}
ggplot(b_and_e_top, aes(x = offence)) +
  geom_bar(color="darksalmon", fill="coral") + 
  labs(x = "B&E Offence Type", y = "Number of Occurrences") +
  coord_flip() +
  ggtitle("Frecuency of Each Type of Offence in 
          High-Risk Neighborhoods")
```

## Results--Subquestion 3: Hypothesis Test 
In order to determine if this specific offence type occurs more often in high-risk than low-risk neighborhoods, we conduct a *hypothesis test*:  
$H_0$: $P_\text{diff}$ = 0  
$H_a$: $P_\text{diff}$ $\neq$ 0   
where $P_\text{diff}$ represents the difference between the proportion of B&E offences in high-risk neighborhoods and that of low-risk neighborhoods.

## Results--Subquestion 3: Hypothesis Test 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=0, fig.height=0}
prop_bande <- b_and_e_extremes %>% 
  group_by(level) %>%
  summarise(n = n(), num_bne = sum(offence == "B&E"), prop_bne = num_bne/n)
test_stat_q3 <- prop_bande %>%
  summarise(test_stat_q3 = diff(prop_bne))
set.seed(123)
repetitions3 <- 1000
simulated_differences_q3 <- rep(NA, times=repetitions3)
for(i in 1:repetitions3){
  simdata_q3 <- b_and_e_extremes %>%
    mutate(level=sample(level)) %>%
    group_by(level) %>%
    summarise(n = n(), num_bne = sum(offence == "B&E"), prop_bne = num_bne/n)
  sim_prop_diff_q3 <- simdata_q3 %>% summarise(value = diff(prop_bne))
  simulated_differences_q3[i] <- as.numeric(sim_prop_diff_q3)
}
sim_q3 <- tibble(prop_diff_q3 = simulated_differences_q3)
```

\tiny Given this sampling distribution, we compare them with the test statistic, which is the proportion of B&E offences over the total number of offences as calculated from our sample data. We want to see if the test statistic is unusual under the null.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2.5}
ggplot(sim_q3, aes(x=prop_diff_q3)) +
geom_histogram(binwidth = 0.005, fill="gray", color="black") +
labs(x = "Difference in proportion of B&E (offence) for high-risk versus
     low-risk neighborhoods")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=0, fig.height=0}
pval3 <- sim_q3 %>% 
  filter(prop_diff_q3 >= abs(test_stat_q3) | prop_diff_q3 <= -abs(test_stat_q3))%>%
  summarise(pvalue3 = n() / repetitions)
```
\tiny Thus, our observed value used for the test is very unusual compared to the simulated ones.  

* Since the p-value is less than 0.0001, it means that there is a **low possibility** that there is data **as unusual as our observed value (test statistic)** under the null hypothesis.  

* Therefore, we have **strong** evidence against the null that there is no difference in the proportion of B&E offences between high-risk and low-risk neighborhoods.


## Results--Subquestion 3 - Boxplot: 
Next we will show how the proportion of type Break and Enters differs between high and low rate neighbourhoods on a plot: 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2.5}
ggplot(b_and_e_extremes, aes(x=level, y=prop)) +
  geom_boxplot() + theme_bw() + ggtitle("Relationship of proportion of B&E offence type crimes in levels of different risk neighbouroods")
```

It is evident that the proportions of break and enters in high-rate neighbourhoods have a much larger range than in low-rate neighbourhoods of this particular type of offence. 

## Results--Subquestion 3 multilinear regression results: 
\tiny Further, we will use another method - a multilinear regresssion model to predict the proportion of Break and Enters using the offence types and level as predictors. The regression equation will be: 
y$_{i}$=$\beta _{0}$+$\beta _{1}$x$_{1i}$ +$\beta _{1}$x$_{2i}$

\tiny where our response variable is y for the ith observation, x$_{1i}$ and x$_{2i}$ are the independent variables, $\beta B_{0}$ is the intercept parameter and $\beta B_{1}$ is the slope parameter.
We are interested to test the
$H_0$: $P_\text{diff}$ = 0
$H_a$: $P_\text{diff}$ != 0
where $H_0$ represents the null hypothesis, $H_a$ the alternative hypothesis, and $P_\text{diff}$ is difference between the proportion of a certain offence type in B&E rated neighborhoods.
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=2.5}
parallel_lines <- lm(prop ~ offence + level, data = b_and_e_extremes)
summary(parallel_lines)$coefficients
```

## Results--Subquestion 3 multilinear regression visual: 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=2.5}
library(broom) 
# augment(parallel_lines)
b_and_e_extremes %>% ggplot(aes(x=offence, y=prop, color=level))+geom_point(alpha=0.5) + geom_line(data=augment(parallel_lines), aes(y=.fitted, colour=level), lwd=1.5) + coord_flip() + ggtitle("Proportion of each type of B&E's in level of risk neighbourhoods")
```
\tiny Based on the fitted regression model, the corresponding p-value test is very small (P = 0.0), so we have very strong evidence to reject the null hypothesis that there is no difference between  proportions of B&E offence types. So we can conclude that there is a difference of proportion between the level and B&E offence types. 


## Conclusion

Give your main conclusions here. Follow the order of questions you presented. 

Here you can also mention any additional considerations, concerns, or issues you might have. For example, if the results were unexpected, you can discuss this and perhaps offer possible explanations.

## Acknowledgements (optional)

If you received any help from someone other than your team members you can acknowledge them. For example:   
*The authors would like to thank "TA name" for helpful suggestions and comments that improved the presentation of this poster.*
